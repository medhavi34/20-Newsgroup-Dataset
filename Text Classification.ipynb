{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/Lenovo/Desktop/34/Coding Ninjas/Bayes Naive/20_newsgroups'\n",
    "folders = [f for f in listdir(path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the list name of the of folders present in 20 newsgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will try to build the pathway to each individual document in our folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [] #adding the name of the individual classes to our original path\n",
    "for i in folders:\n",
    "    all_files = os.path.join(path, i)\n",
    "    files.append([f for f in listdir(all_files)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(files[0]) for i in range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = [] #making a list of all the files in all our individual documents \n",
    "for i in folders: \n",
    "    all_files = join(path, i)\n",
    "    num_files = len(listdir(all_files))\n",
    "    for j in range(num_files):\n",
    "        genre.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pathways = [] #creating a pathway to each individual document\n",
    "for i in folders:\n",
    "    file_num = listdir(join(path, i))\n",
    "    for j in file_num:\n",
    "        all_pathways.append(os.path.join(path,os.path.join(i,j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pathways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(all_pathways, genre,random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions that will be implemented to extract words from text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have stopwords, which are words that add no meaningful value to the sentence; hence, will be removed from all our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "787"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\n",
    "             \"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\n",
    "             \"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\n",
    "             \"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\n",
    "             \"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\n",
    "             \"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\n",
    "             \"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\n",
    "             \"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\n",
    "             \"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\n",
    "             \"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\n",
    "             \"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\n",
    "             \"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\n",
    "             \"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\n",
    "             \"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\"\n",
    "             ,\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\n",
    "             \"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\n",
    "             \"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\n",
    "             \"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\n",
    "             \"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\n",
    "             \"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\n",
    "             \"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\n",
    "             \"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\n",
    "             \"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\n",
    "             \"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\n",
    "             \"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\n",
    "             \"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\n",
    "             \"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\n",
    "             \"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\n",
    "             \"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\n",
    "             \"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\n",
    "             \"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\n",
    "             \"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\"\n",
    "             ,\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\n",
    "             \"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\n",
    "             \"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\n",
    "             \"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\n",
    "             \"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\"\n",
    "             ,\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\n",
    "             \"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\n",
    "             \"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\n",
    "             \"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\n",
    "             \"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\n",
    "             \"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\n",
    "             \"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\n",
    "             \"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\n",
    "             \"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\n",
    "             \"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\n",
    "             \"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\n",
    "             \"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\n",
    "             \"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\n",
    "             \"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\n",
    "             \"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\n",
    "             \"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\n",
    "             \"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\n",
    "             \"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\n",
    "             \"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\n",
    "             \"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\n",
    "             \"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\n",
    "             \"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\n",
    "             \"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\",\"n't\",\"m\"]\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the document will use alot of punctuations, captilization, stopwords, and tabs, we will use the following function to make our words list more clean and accurate. We will be using the translate function as this allows us to delete characters not required in the list by the third argument. We will also we using .lower() to nomralize our words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helps in the extraction of words\n",
    "def preprocess(words):\n",
    "    \n",
    "    #first, we normalize the cases of our words\n",
    "    words = [word.lower() for word in words]\n",
    "    #then, we will remove all the stopwords present in our document\n",
    "    words = [word for word in words if not word in stopwords]\n",
    "    #then we will try to filter out some  unnecessary data like tabs, punctuations, etc.\n",
    "    table = str.maketrans('', '', '\\t')\n",
    "    words = [word.translate(table) for word in words]\n",
    "    table = str.maketrans('', '', string.punctuation) #string.punctuations refer to !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "    words = [word.translate(table) for word in words]\n",
    "    #some white spaces may be added to the list of words, due to the translate function & nature of our documents\n",
    "    #we remove them below\n",
    "    words = [str for str in words if str]\n",
    "    #we will also remove just-numeric strings as they do not have any significant meaning in text classification\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "    #we will also remove words with only 2 characters\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    #after removal of so many characters it may happen that some strings have become blank, we remove those\n",
    "    words = [str for str in words if str]  \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function removes all the meta-data on the top of each text file which refers to subject line etc. \n",
    "def remove_metadata(lines):\n",
    "    for i in range(len(lines)):\n",
    "        if(lines[i] == '\\n'):\n",
    "            start = i+1\n",
    "            break\n",
    "    return lines[start:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(line):\n",
    "    words = line[0:len(line)-1].strip().split(\" \")\n",
    "    return preprocess(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(path):\n",
    "    #load document as a list of lines\n",
    "    f = open(path, 'r')\n",
    "    text_lines = f.readlines()\n",
    "    text_lines = remove_metadata(text_lines)\n",
    "    words = []\n",
    "    for line in text_lines:\n",
    "        words.append(tokenize_sentence(line))\n",
    "    #words is a 2D list each list representing a single document \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(doc_words):\n",
    "    return [val for sublists in doc_words for val in sublists] #flattens our 2D file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Above Functions to Calculate the Number of Words in all the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using document in X_train which is a list of paths of the files, we will find the list of words present in our train document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_words(X_train):\n",
    "    list_of_words=[]\n",
    "    for document in X_train:\n",
    "        list_of_words.append(flatten(tokenize(document)))\n",
    "    return list_of_words, flatten(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_of_words_train, flattened_list_train = list_of_words(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14997"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_words_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1666923"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flattened_list_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Above Data and Finding Out the Frequencies of Different Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.asarray(flattened_list_train)\n",
    "unique, count = np.unique(words, return_counts = True) #gets all theunique words in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141230"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sort_ind = np.argsort(-count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.arange(len(unique))\n",
    "counts = count[count_sort_ind]\n",
    "words = unique[count_sort_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlim(0,250)\n",
    "plt.plot(points,counts)\n",
    "plt.xlabel('No. of words')\n",
    "plt.ylabel('Frequencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be using the 5000 most frequent words from this data that we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we form the dictionary through this whcih tells us the freqency of each word. \n",
    "def dictionary(list_of_words):\n",
    "    dictionary = {}\n",
    "    num = 1\n",
    "    for words in list_of_words:\n",
    "        np_words = np.asarray(words)\n",
    "        w,c = np.unique(np_words, return_counts = True)\n",
    "        dictionary[num] = {}\n",
    "        for i in range(len(w)):\n",
    "            dictionary[num][w[i]] = c[i]\n",
    "        num += 1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we find the frequency of each word in the dictionary in X \n",
    "def words_in_doc(features, dictionary):\n",
    "    X = []\n",
    "    for words in dictionary.keys():\n",
    "        feature_of_word = dictionary[words].keys()\n",
    "        row = []\n",
    "        for feature in features:\n",
    "            if feature in feature_of_word:\n",
    "                row.append(dictionary[words][feature])\n",
    "            else:\n",
    "                row.append(0)\n",
    "        X.append(row)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = words[0:5000] #take 5000 most repeated words. \n",
    "dic = dictionary(list_of_words_train) #gives us dic\n",
    "Y_train = np.asarray(Y_train)\n",
    "X_train = np.asarray(words_in_doc(features, dic)) #gives us X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14997, 5000), (14997,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to do the same for test dataset as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words_test, flattened_list_test = list_of_words(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 578932)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_words_test), len(flattened_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_test = dictionary(list_of_words_test)\n",
    "Y_test = np.asarray(Y_test)\n",
    "X_test = np.asarray(words_in_doc(features, dic_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 5000), (5000,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkLearn Multinomial Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.769"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict = clf.predict(X_test)\n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.61      0.75      0.67       233\n",
      "           comp.graphics       0.61      0.69      0.65       253\n",
      " comp.os.ms-windows.misc       0.75      0.66      0.70       249\n",
      "comp.sys.ibm.pc.hardware       0.66      0.72      0.69       240\n",
      "   comp.sys.mac.hardware       0.73      0.77      0.75       236\n",
      "          comp.windows.x       0.79      0.74      0.77       240\n",
      "            misc.forsale       0.80      0.71      0.75       261\n",
      "               rec.autos       0.79      0.83      0.81       269\n",
      "         rec.motorcycles       0.82      0.89      0.86       284\n",
      "      rec.sport.baseball       0.89      0.91      0.90       248\n",
      "        rec.sport.hockey       0.87      0.94      0.90       231\n",
      "               sci.crypt       0.94      0.87      0.90       233\n",
      "         sci.electronics       0.76      0.70      0.72       244\n",
      "                 sci.med       0.90      0.85      0.88       256\n",
      "               sci.space       0.88      0.83      0.85       246\n",
      "  soc.religion.christian       0.78      0.85      0.82       252\n",
      "      talk.politics.guns       0.69      0.84      0.76       249\n",
      "   talk.politics.mideast       0.90      0.84      0.87       281\n",
      "      talk.politics.misc       0.64      0.62      0.63       259\n",
      "      talk.religion.misc       0.57      0.35      0.44       236\n",
      "\n",
      "                accuracy                           0.77      5000\n",
      "               macro avg       0.77      0.77      0.77      5000\n",
      "            weighted avg       0.77      0.77      0.77      5000\n",
      "\n",
      "[[174   1   0   0   0   0   0   0   3   1   0   0   1   2   0  14   4   1\n",
      "    7  25]\n",
      " [  3 175  21  16   7  13   2   1   1   0   1   1   2   3   3   2   0   0\n",
      "    1   1]\n",
      " [  0  23 164  20   7  19   4   0   1   1   2   0   3   0   3   0   0   0\n",
      "    2   0]\n",
      " [  0  10  13 174  24   0   9   4   0   0   0   0   6   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   1   5  20 181   2   9   3   0   0   3   0   7   3   0   0   0   0\n",
      "    0   1]\n",
      " [  0  36   9   2   2 178   0   0   3   0   1   1   2   1   1   0   0   1\n",
      "    2   1]\n",
      " [  0   1   1  11   9   2 186  16   5   2   6   2  12   0   4   0   1   0\n",
      "    2   1]\n",
      " [  0   2   0   0   3   1   8 222  11   3   2   0   7   1   3   0   5   0\n",
      "    0   1]\n",
      " [  0   2   1   1   2   0   2  14 254   0   0   0   3   2   0   0   0   0\n",
      "    3   0]\n",
      " [  0   3   0   0   0   1   2   3   1 225   5   0   0   2   1   1   4   0\n",
      "    0   0]\n",
      " [  0   0   1   0   0   0   0   0   4   7 217   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   4   2   0   1   5   1   0   1   0   0 203   1   1   0   0   4   0\n",
      "    9   0]\n",
      " [  0  12   2  13  11   2   5   7   4   4   2   5 170   2   3   1   0   1\n",
      "    0   0]\n",
      " [  3   6   0   0   1   0   0   3   6   2   2   0   5 217   5   2   2   1\n",
      "    0   1]\n",
      " [  4   5   0   2   0   2   1   2   6   2   2   0   3   1 203   0   2   0\n",
      "   10   1]\n",
      " [ 14   1   1   0   0   0   1   0   2   2   1   0   1   1   1 214   2   2\n",
      "    4   5]\n",
      " [  2   1   0   0   0   0   0   3   0   1   2   1   1   1   0   0 209   2\n",
      "   17   9]\n",
      " [  7   0   0   2   1   0   2   3   3   0   2   0   0   0   0   4   3 236\n",
      "   15   3]\n",
      " [  6   1   0   2   0   0   0   1   1   2   1   3   0   2   4   3  44  16\n",
      "  160  13]\n",
      " [ 70   1   0   1   0   0   0   0   2   1   1   0   0   1   1  32  23   3\n",
      "   17  83]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Y_test,Y_predict))\n",
    "print(confusion_matrix(Y_test,Y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8343001933720078"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = clf.predict(X_train)\n",
    "clf.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.71      0.86      0.78       767\n",
      "           comp.graphics       0.70      0.79      0.74       747\n",
      " comp.os.ms-windows.misc       0.82      0.78      0.80       751\n",
      "comp.sys.ibm.pc.hardware       0.77      0.80      0.78       760\n",
      "   comp.sys.mac.hardware       0.81      0.87      0.84       764\n",
      "          comp.windows.x       0.88      0.80      0.84       760\n",
      "            misc.forsale       0.86      0.82      0.84       739\n",
      "               rec.autos       0.89      0.89      0.89       731\n",
      "         rec.motorcycles       0.85      0.94      0.90       716\n",
      "      rec.sport.baseball       0.94      0.94      0.94       752\n",
      "        rec.sport.hockey       0.90      0.95      0.92       769\n",
      "               sci.crypt       0.92      0.90      0.91       767\n",
      "         sci.electronics       0.85      0.78      0.81       756\n",
      "                 sci.med       0.93      0.88      0.91       744\n",
      "               sci.space       0.93      0.89      0.91       754\n",
      "  soc.religion.christian       0.84      0.89      0.87       745\n",
      "      talk.politics.guns       0.75      0.89      0.81       751\n",
      "   talk.politics.mideast       0.91      0.86      0.88       719\n",
      "      talk.politics.misc       0.75      0.70      0.72       741\n",
      "      talk.religion.misc       0.74      0.46      0.57       764\n",
      "\n",
      "                accuracy                           0.83     14997\n",
      "               macro avg       0.84      0.83      0.83     14997\n",
      "            weighted avg       0.84      0.83      0.83     14997\n",
      "\n",
      "[[658   2   0   0   0   0   0   2   7   0   2   1   3   3   3  28   5   8\n",
      "    4  41]\n",
      " [  4 589  35  27  18  30   9   4   4   0   5   3   8   4   2   2   1   1\n",
      "    0   1]\n",
      " [  1  39 584  48  14  25   7   3   2   2  10   3   4   1   1   1   0   1\n",
      "    1   4]\n",
      " [  1  17  30 610  43   6  11   2   4   2   4   4  21   2   1   0   0   0\n",
      "    1   1]\n",
      " [  1  13  21  26 667   2  11   3   3   0   2   3   8   2   1   0   0   0\n",
      "    1   0]\n",
      " [  1  85  15  13   7 607   3   0   1   1   5   2   5   1   6   0   3   1\n",
      "    2   2]\n",
      " [  1   5   8  32  20   0 607  17  11   2   9   2   9   2   6   1   2   0\n",
      "    4   1]\n",
      " [  2   5   1   3   4   0  12 647  21   4   4   2  13   2   2   1   4   0\n",
      "    3   1]\n",
      " [  2   4   0   0   0   0   8  12 675   3   3   0   4   1   0   0   4   0\n",
      "    0   0]\n",
      " [  2   4   0   0   0   3   3   2   8 705  19   0   2   1   0   0   1   1\n",
      "    0   1]\n",
      " [  1   2   1   0   0   2   3   1   5  13 729   2   1   2   0   1   2   0\n",
      "    3   1]\n",
      " [  2   7   8   3   6   1   2   0   2   0   2 694   8   1   2   0  13   1\n",
      "   11   4]\n",
      " [  2  23   5  27  37   4  20  17   4   5   2   9 590   2   2   0   0   1\n",
      "    5   1]\n",
      " [  6  15   1   1   5   0   6   3  10   4   4   1  12 654   4   3   6   0\n",
      "    9   0]\n",
      " [  5  23   0   1   1   1   2   4   9   3   1   1   5   5 672   5   2   1\n",
      "    9   4]\n",
      " [ 32   1   2   1   4   1   2   0   2   1   1   0   1   5   3 663   7   6\n",
      "    3  10]\n",
      " [  3   4   2   0   0   2   1   1   6   0   2   7   1   2   2   2 667   4\n",
      "   33  12]\n",
      " [  7   4   0   3   1   1   1   3   6   2   2   7   0   2   3   9   8 618\n",
      "   39   3]\n",
      " [ 14   1   1   1   0   1   1   3   4   4   2   9   0   5   8   7  92  30\n",
      "  521  37]\n",
      " [184   4   0   1   0   1   0   2   8   1   2   1   3   4   2  63  77   6\n",
      "   50 355]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Y_train,Y_pred))\n",
    "print(confusion_matrix(Y_train,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Text Classification by writing Multinomial Naive Bayes by Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function forms our dictionary which tells us the frequency of each word in each class, and total words in the document and class.\n",
    "def fit(X, Y, features):\n",
    "    dictionary = {}\n",
    "    dictionary['total_data'] = len(flatten(X))\n",
    "    keys = set(Y)\n",
    "    for doc_name in keys:\n",
    "        dictionary[doc_name] = {}\n",
    "        usable_cases = (Y == doc_name)\n",
    "        X_cases = X[usable_cases]\n",
    "        dictionary[doc_name]['total_count'] = len(flatten(X_cases))\n",
    "        for case in range(1, X.shape[1]):\n",
    "            dictionary[doc_name][features[case - 1]] = X_cases[:, case - 1].sum()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probability of each class using laplace correction \n",
    "def probability(x, curr_class, dic):\n",
    "    output = np.log(dic[curr_class][\"total_count\"]) - np.log(dic[\"total_data\"])\n",
    "    for j in range(len(x) - 1):\n",
    "        if x[j] in dic[curr_class].keys():\n",
    "            xj = x[j]\n",
    "            count_curr_class_equal_xj = dic[curr_class][xj] + 1\n",
    "            count_curr_class = dic[curr_class][\"total_count\"] + len(dic[curr_class].keys())\n",
    "            curr_xj_prob = np.log(count_curr_class_equal_xj) - np.log(count_curr_class)\n",
    "            output += curr_xj_prob\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this helps in deciding the most accurate class. \n",
    "def predictSinglePoint(x, dic):\n",
    "    classes = dic.keys()\n",
    "    best_prob = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for i in classes:\n",
    "        if i == \"total_data\":\n",
    "            continue\n",
    "        p_curr_class = probability(x, i, dic)\n",
    "        if((first_run) or (p_curr_class > best_prob)):\n",
    "            best_prob = p_curr_class\n",
    "            best_class = i\n",
    "            first_run = False\n",
    "    return (best_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dic, X):\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        prob = predictSinglePoint(x, dic)\n",
    "        y_pred.append(prob)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = fit(X_train, Y_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "\n",
    "for i in dic_test:\n",
    "    X_test.append(list(dic_test[i].keys()))\n",
    "#tells us all the words present in each document in X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(dic, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.64      0.63      0.63       233\n",
      "           comp.graphics       0.49      0.65      0.56       253\n",
      " comp.os.ms-windows.misc       0.85      0.32      0.47       249\n",
      "comp.sys.ibm.pc.hardware       0.64      0.63      0.63       240\n",
      "   comp.sys.mac.hardware       0.92      0.46      0.61       236\n",
      "          comp.windows.x       0.61      0.75      0.67       240\n",
      "            misc.forsale       0.84      0.43      0.57       261\n",
      "               rec.autos       0.80      0.49      0.61       269\n",
      "         rec.motorcycles       0.99      0.47      0.64       284\n",
      "      rec.sport.baseball       0.97      0.65      0.78       248\n",
      "        rec.sport.hockey       0.80      0.86      0.83       231\n",
      "               sci.crypt       0.62      0.84      0.71       233\n",
      "         sci.electronics       0.76      0.41      0.53       244\n",
      "                 sci.med       0.93      0.68      0.79       256\n",
      "               sci.space       0.80      0.70      0.74       246\n",
      "  soc.religion.christian       0.68      0.87      0.76       252\n",
      "      talk.politics.guns       0.75      0.51      0.60       249\n",
      "   talk.politics.mideast       0.50      0.93      0.65       281\n",
      "      talk.politics.misc       0.24      0.81      0.38       259\n",
      "      talk.religion.misc       0.46      0.22      0.29       236\n",
      "\n",
      "                accuracy                           0.62      5000\n",
      "               macro avg       0.71      0.62      0.62      5000\n",
      "            weighted avg       0.71      0.62      0.62      5000\n",
      "\n",
      "[[146   0   0   0   0   1   0   0   0   0   0   1   0   0   0  21   1  16\n",
      "   19  28]\n",
      " [  2 164   2   9   0  28   0   0   0   0   0   9   1   1   6   4   0   8\n",
      "   18   1]\n",
      " [  1  44  80  16   1  59   0   0   0   0   2  14   2   0   5   2   0   2\n",
      "   21   0]\n",
      " [  0  27   5 151   1  10   6   0   0   0   1  16   3   1   2   1   0   3\n",
      "   12   1]\n",
      " [  1  22   1  29 108   7   5   0   0   0   5  13   8   3   1   0   2   4\n",
      "   26   1]\n",
      " [  2  28   3   1   0 181   1   0   0   0   2   5   0   1   1   0   0   4\n",
      "   11   0]\n",
      " [  0  15   1  21   6   0 113  14   1   0   7   7  12   1  10   0   1  19\n",
      "   32   1]\n",
      " [  0   3   0   0   0   1   6 133   0   1   4   4   3   2   4   1   5  31\n",
      "   68   3]\n",
      " [  4   2   0   1   0   0   2  16 134   0   1   2   1   1   0   0   3  30\n",
      "   84   3]\n",
      " [  2   4   0   0   0   1   0   0   0 162  16   1   0   2   1   2   3  15\n",
      "   37   2]\n",
      " [  0   0   0   0   0   0   0   0   0   2 198   1   0   0   0   1   0   6\n",
      "   22   1]\n",
      " [  1   2   1   0   0   3   0   0   0   0   0 195   0   0   0   1   3   2\n",
      "   25   0]\n",
      " [  1  16   1   9   2   6   1   3   0   0   3  36  99   1   9   3   1  16\n",
      "   36   1]\n",
      " [  6   5   0   0   0   0   0   0   0   0   2   1   0 175   3   5   3  24\n",
      "   29   3]\n",
      " [  5   3   0   0   0   1   1   0   0   0   2   3   1   0 172   1   1   9\n",
      "   44   3]\n",
      " [  3   1   0   0   0   0   0   0   0   1   0   1   0   0   0 219   0  11\n",
      "   11   5]\n",
      " [  0   0   0   0   0   0   0   0   0   0   2   3   0   1   0   3 126  18\n",
      "   94   2]\n",
      " [  1   0   0   0   0   1   0   0   1   0   0   1   0   0   0   2   1 261\n",
      "   11   2]\n",
      " [  1   0   0   0   0   0   0   0   0   1   1   2   0   0   1   2   7  30\n",
      "  211   3]\n",
      " [ 53   1   0   0   0   0   0   0   0   0   1   1   0   0   1  55  11  11\n",
      "   51  51]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Y_test,Y_pred))\n",
    "print(confusion_matrix(Y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
